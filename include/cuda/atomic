/*

Copyright (c) 2017, NVIDIA Corporation
All rights reserved.

Redistribution and use in source and binary forms, with or without modification,
are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this
list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright notice,
this list of conditions and the following disclaimer in the documentation
and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
OF THE POSSIBILITY OF SUCH DAMAGE.

*/

#ifndef __CUDA_ATOMIC_HPP__
#define __CUDA_ATOMIC_HPP__

#include "details/config.hpp"

#include <cassert>
#include <algorithm>
#include <thread>
#include <chrono>
#include <atomic>
#include <type_traits>

namespace cuda { namespace experimental { inline namespace v1 {

    namespace details {

        template<class E>
        __device__ E __stronger_order(E a, E b) {
            if (b == std::memory_order_seq_cst) return std::memory_order_seq_cst;
            if (b == std::memory_order_relaxed) return a;
            switch (a) {
            case std::memory_order_seq_cst:
            case std::memory_order_acq_rel: return a;
            case std::memory_order_consume:
            case std::memory_order_acquire: if (b != std::memory_order_acquire) return std::memory_order_acq_rel; else return std::memory_order_acquire;
            case std::memory_order_release: if (b != std::memory_order_release) return std::memory_order_acq_rel; else return std::memory_order_release;
            case std::memory_order_relaxed: return b;
            default: assert(0);
            }
            return std::memory_order_seq_cst;
        }

        template<class T>
        using __stripped_pointer_type = typename std::remove_cv<typename std::remove_pointer<T>::type>::type;

        template<class T, typename std::enable_if<std::is_pointer<T>::value && !std::is_same<__stripped_pointer_type<T>, void>::value, int>::type = 0>
        __device__ size_t __scale_addend() {
            return sizeof(__stripped_pointer_type<T>);
        }
        template<class T, typename std::enable_if<!std::is_pointer<T>::value || std::is_same<__stripped_pointer_type<T>, void>::value, int>::type = 0>
        __device__ size_t __scale_addend() {
            return 1;
        }

        template <class A>
        __device__ inline intptr_t __semaphore_alignerr(A& a) {
            return ((intptr_t)&a & (sizeof(int) - 1));
        }
        template <class A>
        __device__ inline void* __semaphore_fixalign(A& a) {
            return (void*)((intptr_t)&a & ~(sizeof(int) - 1));
        }

        __device__ static inline void __mme_fence_signal() noexcept {
            __mme_fence_signal_();
        }
        __device__ static inline void __mme_fence_sc() noexcept {
            __mme_fence_sc_();
        }
        __device__ static inline void __mme_fence() noexcept {
            __mme_fence_();
        }

        template<size_t N>
        struct __mme;

        template<>
        struct __mme<1> {

            template<class T> __device__ static inline void load_acquire(T const& ref, T& ret) noexcept {
                int32_t temp = 0;
                __mme_load_acquire_8_as_32(&ref, temp);
                ret = temp;
            }
            template<class T> __device__ static inline void load_relaxed(T const& ref, T& ret) noexcept {
                int32_t temp = 0;
                __mme_load_relaxed_8_as_32(&ref, temp);
                ret = temp;
            }

            template<class T> __device__ static inline void store_release(T& ref, T desired) noexcept {
                int32_t const temp = desired;
                __mme_store_release_8_as_32(&ref, temp);
            }
            template<class T> __device__ static inline void store_relaxed(T& ref, T desired) noexcept {
                int32_t const temp = desired;
                __mme_store_relaxed_8_as_32(&ref, temp);
            }
#ifdef __has_cuda_mmio
            template<class T> __device__ static inline void load_mmio(T const volatile& ref, T& ret) noexcept {
                int32_t temp = 0;
                __mme_load_relaxed_mmio_8_as_32(&ref, temp);
                ret = temp;
            }
            template<class T> __device__ static inline void store_mmio(T volatile& ref, T desired) noexcept {
                int32_t const temp = desired;
                __mme_store_relaxed_mmio_8_as_32(&ref, temp);
            }
#endif
        };

        template<>
        struct __mme<2> {

            template<class T> __device__ static inline void load_acquire(T const& ptr, T& ret) noexcept {
                __mme_load_acquire_16(&ptr,ret);
            }
            template<class T> __device__ static inline void load_relaxed(T const& ptr, T& ret) noexcept {
                __mme_load_relaxed_16(&ptr,ret);
            }

            template<class T> __device__ static inline void store_release(T& ptr, T desired) noexcept {
                __mme_store_release_16(&ptr,desired);
            }
            template<class T> __device__ static inline void store_relaxed(T& ptr, T desired) noexcept {
                __mme_store_relaxed_16(&ptr,desired);
            }
#ifdef __has_cuda_mmio
            template<class T> __device__ static inline void load_mmio(T const volatile& ptr, T& ret) noexcept {
                __mme_load_relaxed_mmio_16(&ptr,ret);
            }
            template<class T> __device__ static inline void store_mmio(T volatile& ptr, T desired) noexcept {
                __mme_store_relaxed_mmio_16(&ptr,desired);
            }
#endif
        };

        template<>
        struct __mme<4> {

            template<class T> __device__ static inline void load_acquire(T const& ptr, T& ret) noexcept {
                __mme_load_acquire_32(&ptr, ret);
            }
            template<class T> __device__ static inline void load_relaxed(T const& ptr, T& ret) noexcept {
                __mme_load_relaxed_32(&ptr, ret);
            }

            template<class T> __device__ static inline void store_release(T& ptr, T desired) noexcept {
                __mme_store_release_32(&ptr, desired);
            }
            template<class T> __device__ static inline void store_relaxed(T& ptr, T desired) noexcept {
                __mme_store_relaxed_32(&ptr, desired);
            }

            template<class T> __device__ static inline void exch_release(T& ptr, T& old, T desired) noexcept {
                __mme_exch_release_32(&ptr, old, desired);
            }
            template<class T> __device__ static inline void exch_acquire(T& ptr, T& old, T desired) noexcept {
                __mme_exch_acquire_32(&ptr, old, desired);
            }
            template<class T> __device__ static inline void exch_acq_rel(T& ptr, T& old, T desired) noexcept {
                __mme_exch_acq_rel_32(&ptr, old, desired);
            }
            template<class T> __device__ static inline void exch_relaxed(T& ptr, T& old, T desired) noexcept {
                __mme_exch_relaxed_32(&ptr, old, desired);
            }

            template<class T> __device__ static inline void cas_release(T& ptr, T& old, T expected, T desired) noexcept {
                __mme_cas_release_32(&ptr,old,expected,desired);
            }
            template<class T> __device__ static inline void cas_acquire(T& ptr, T& old, T expected, T desired) noexcept {
                __mme_cas_acquire_32(&ptr,old,expected,desired);
            }
            template<class T> __device__ static inline void cas_acq_rel(T& ptr, T& old, T expected, T desired) noexcept {
                __mme_cas_acq_rel_32(&ptr,old,expected,desired);
            }
            template<class T> __device__ static inline void cas_relaxed(T& ptr, T& old, T expected, T desired) noexcept {
                __mme_cas_relaxed_32(&ptr,old,expected,desired);
            }

            template<class T> __device__ static inline void add_release(T& ptr, T& old, T addend) noexcept {
                __mme_add_release_32(&ptr,old,addend);
            }
            template<class T> __device__ static inline void add_acquire(T& ptr, T& old, T addend) noexcept {
                __mme_add_acquire_32(&ptr,old,addend);
            }
            template<class T> __device__ static inline void add_acq_rel(T& ptr, T& old, T addend) noexcept {
                __mme_add_acq_rel_32(&ptr,old,addend);
            }
            template<class T> __device__ static inline void add_relaxed(T& ptr, T& old, T addend) noexcept {
                __mme_add_relaxed_32(&ptr,old,addend);
            }

            template<class T> __device__ static inline void and_release(T& ptr, T& old, T andend) noexcept {
                __mme_and_release_32(&ptr,old,andend);
            }
            template<class T> __device__ static inline void and_acquire(T& ptr, T& old, T andend) noexcept {
                __mme_and_acquire_32(&ptr,old,andend);
            }
            template<class T> __device__ static inline void and_acq_rel(T& ptr, T& old, T andend) noexcept {
                __mme_and_acq_rel_32(&ptr,old,andend);
            }
            template<class T> __device__ static inline void and_relaxed(T& ptr, T& old, T andend) noexcept {
                __mme_and_relaxed_32(&ptr,old,andend);
            }

            template<class T> __device__ static inline void or_release(T& ptr, T& old, T orend) noexcept {
                __mme_or_release_32(&ptr,old,orend);
            }
            template<class T> __device__ static inline void or_acquire(T& ptr, T& old, T orend) noexcept {
                __mme_or_acquire_32(&ptr,old,orend);
            }
            template<class T> __device__ static inline void or_acq_rel(T& ptr, T& old, T orend) noexcept {
                __mme_or_acq_rel_32(&ptr,old,orend);
            }
            template<class T> __device__ static inline void or_relaxed(T& ptr, T& old, T orend) noexcept {
                __mme_or_relaxed_32(&ptr,old,orend);
            }

            template<class T> __device__ static inline void xor_release(T& ptr, T& old, T xorend) noexcept {
                __mme_xor_release_32(&ptr,old,xorend);
            }
            template<class T> __device__ static inline void xor_acquire(T& ptr, T& old, T xorend) noexcept {
                __mme_xor_acquire_32(&ptr,old,xorend);
            }
            template<class T> __device__ static inline void xor_acq_rel(T& ptr, T& old, T xorend) noexcept {
                __mme_xor_acq_rel_32(&ptr,old,xorend);
            }
            template<class T> __device__ static inline void xor_relaxed(T& ptr, T& old, T xorend) noexcept {
                __mme_xor_relaxed_32(&ptr,old,xorend);
            }

#ifdef __has_cuda_mmio
            template<class T> __device__ static inline void load_mmio(T const volatile& ptr, T& ret) noexcept {
                __mme_load_relaxed_mmio_32(&ptr, ret);
            }
            template<class T> __device__ static inline void store_mmio(T volatile& ptr, T desired) noexcept {
                __mme_store_relaxed_mmio_32(&ptr, desired);
            }
            template<class T> __device__ static inline void exch_mmio(T volatile& ptr, T& old, T desired) noexcept {
                __mme_exch_relaxed_mmio_32(&ptr, old, desired);
            }
            template<class T> __device__ static inline void cas_mmio(T volatile& ptr, T& old, T expected, T desired) noexcept {
                __mme_cas_relaxed_mmio_32(&ptr,old,expected,desired);
            }
            template<class T> __device__ static inline void add_mmio(T volatile& ptr, T& old, T addend) noexcept {
                __mme_add_relaxed_mmio_32(&ptr,old,addend);
            }
            template<class T> __device__ static inline void and_mmio(T volatile& ptr, T& old, T andend) noexcept {
                __mme_and_release_32(&ptr,old,andend);
            }
            template<class T> __device__ static inline void or_mmio(T volatile& ptr, T& old, T orend) noexcept {
                __mme_or_release_32(&ptr,old,orend);
            }            
            template<class T> __device__ static inline void xor_mmio(T volatile& ptr, T& old, T xorend) noexcept {
                __mme_xor_release_32(&ptr,old,xorend);
            }
#endif
        };

        template<>
        struct __mme<8> {

            template<class T> __device__ static inline void load_acquire(T const& ptr, T& ret) noexcept {
                __mme_load_acquire_64(&ptr, ret);
            }
            template<class T> __device__ static inline void load_relaxed(T const& ptr, T& ret) noexcept {
                __mme_load_relaxed_64(&ptr, ret);
            }

            template<class T> __device__ static inline void store_release(T& ptr, T desired) noexcept {
                __mme_store_release_64(&ptr, desired);
            }
            template<class T> __device__ static inline void store_relaxed(T& ptr, T desired) noexcept {
                __mme_store_relaxed_64(&ptr, desired);
            }

            template<class T> __device__ static inline void exch_release(T& ptr, T& old, T desired) noexcept {
                __mme_exch_release_64(&ptr, old, desired);
            }
            template<class T> __device__ static inline void exch_acquire(T& ptr, T& old, T desired) noexcept {
                __mme_exch_acquire_64(&ptr, old, desired);
            }
            template<class T> __device__ static inline void exch_acq_rel(T& ptr, T& old, T desired) noexcept {
                __mme_exch_acq_rel_64(&ptr, old, desired);
            }
            template<class T> __device__ static inline void exch_relaxed(T& ptr, T& old, T desired) noexcept {
                __mme_exch_relaxed_64(&ptr, old, desired);
            }

            template<class T> __device__ static inline void cas_release(T& ptr, T& old, T expected, T desired) noexcept {
                __mme_cas_release_64(&ptr,old,expected,desired);
            }
            template<class T> __device__ static inline void cas_acquire(T& ptr, T& old, T expected, T desired) noexcept {
                __mme_cas_acquire_64(&ptr,old,expected,desired);
            }
            template<class T> __device__ static inline void cas_acq_rel(T& ptr, T& old, T expected, T desired) noexcept {
                __mme_cas_acq_rel_64(&ptr,old,expected,desired);
            }
            template<class T> __device__ static inline void cas_relaxed(T& ptr, T& old, T expected, T desired) noexcept {
                __mme_cas_relaxed_64(&ptr,old,expected,desired);
            }

            template<class T> __device__ static inline void add_release(T& ptr, T& old, T addend) noexcept {
                __mme_add_release_64(&ptr,old,addend);
            }
            template<class T> __device__ static inline void add_acquire(T& ptr, T& old, T addend) noexcept {
                __mme_add_acquire_64(&ptr,old,addend);
            }
            template<class T> __device__ static inline void add_acq_rel(T& ptr, T& old, T addend) noexcept {
                __mme_add_acq_rel_64(&ptr,old,addend);
            }
            template<class T> __device__ static inline void add_relaxed(T& ptr, T& old, T addend) noexcept {
                __mme_add_relaxed_64(&ptr,old,addend);
            }

            template<class T> __device__ static inline void and_release(T& ptr, T& old, T andend) noexcept {
                __mme_and_release_64(&ptr,old,andend);
            }
            template<class T> __device__ static inline void and_acquire(T& ptr, T& old, T andend) noexcept {
                __mme_and_acquire_64(&ptr,old,andend);
            }
            template<class T> __device__ static inline void and_acq_rel(T& ptr, T& old, T andend) noexcept {
                __mme_and_acq_rel_64(&ptr,old,andend);
            }
            template<class T> __device__ static inline void and_relaxed(T& ptr, T& old, T andend) noexcept {
                __mme_and_relaxed_64(&ptr,old,andend);
            }

            template<class T> __device__ static inline void or_release(T& ptr, T& old, T orend) noexcept {
                __mme_or_release_64(&ptr,old,orend);
            }
            template<class T> __device__ static inline void or_acquire(T& ptr, T& old, T orend) noexcept {
                __mme_or_acquire_64(&ptr,old,orend);
            }
            template<class T> __device__ static inline void or_acq_rel(T& ptr, T& old, T orend) noexcept {
                __mme_or_acq_rel_64(&ptr,old,orend);
            }
            template<class T> __device__ static inline void or_relaxed(T& ptr, T& old, T orend) noexcept {
                __mme_or_relaxed_64(&ptr,old,orend);
            }

            template<class T> __device__ static inline void xor_release(T& ptr, T& old, T xorend) noexcept {
                __mme_xor_release_64(&ptr,old,xorend);
            }
            template<class T> __device__ static inline void xor_acquire(T& ptr, T& old, T xorend) noexcept {
                __mme_xor_acquire_64(&ptr,old,xorend);
            }
            template<class T> __device__ static inline void xor_acq_rel(T& ptr, T& old, T xorend) noexcept {
                __mme_xor_acq_rel_64(&ptr,old,xorend);
            }
            template<class T> __device__ static inline void xor_relaxed(T& ptr, T& old, T xorend) noexcept {
                __mme_xor_relaxed_64(&ptr,old,xorend);
            }
#ifdef __has_cuda_mmio
            template<class T> __device__ static inline void load_mmio(T const volatile& ptr, T& ret) noexcept {
                __mme_load_relaxed_mmio_64(&ptr, ret);
            }
            template<class T> __device__ static inline void store_mmio(T volatile& ptr, T desired) noexcept {
                __mme_store_relaxed_mmio_64(&ptr, desired);
            }
            template<class T> __device__ static inline void exch_mmio(T volatile& ptr, T& old, T desired) noexcept {
                __mme_exch_relaxed_mmio_64(&ptr, old, desired);
            }
            template<class T> __device__ static inline void cas_mmio(T volatile& ptr, T& old, T expected, T desired) noexcept {
                __mme_cas_relaxed_mmio_64(&ptr,old,expected,desired);
            }
            template<class T> __device__ static inline void add_mmio(T volatile& ptr, T& old, T addend) noexcept {
                __mme_add_relaxed_mmio_64(&ptr,old,addend);
            }
            template<class T> __device__ static inline void and_mmio(T volatile& ptr, T& old, T andend) noexcept {
                __mme_and_release_64(&ptr,old,andend);
            }
            template<class T> __device__ static inline void or_mmio(T volatile& ptr, T& old, T orend) noexcept {
                __mme_or_release_64(&ptr,old,orend);
            }
            template<class T> __device__ static inline void xor_mmio(T volatile& ptr, T& old, T xorend) noexcept {
                __mme_xor_release_64(&ptr,old,xorend);
            }
#endif
        };

        template<class T>
        struct __mme_std : __mme<sizeof(T)> {

            __device__ static inline T load(T const& value, std::memory_order order) noexcept {
                T ret = T();
                switch (order) {
                case std::memory_order_seq_cst: __mme_fence_sc();
                case std::memory_order_consume:
                case std::memory_order_acquire: load_acquire(value, ret); break;
                case std::memory_order_relaxed: load_relaxed(value, ret); break;
                default: assert(0);
                }
                return ret;
            }
            __device__ static inline void store(T& value, T desired, std::memory_order order) noexcept {

                switch (order) {
                case std::memory_order_release: store_release(value, desired); break;
                case std::memory_order_seq_cst: __mme_fence_sc();
                case std::memory_order_relaxed: store_relaxed(value, desired); break;
                default: assert(0);
                }
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2, int>::type = 0>
            __device__ static inline T exchange(T& value, T desired, std::memory_order order) noexcept {
                T old = T();
                switch (order) {
                case std::memory_order_seq_cst: __mme_fence_sc();
                case std::memory_order_consume:
                case std::memory_order_acquire: exch_acquire(value, old, desired); break;
                case std::memory_order_acq_rel: exch_acq_rel(value, old, desired); break;
                case std::memory_order_release: exch_release(value, old, desired); break;
                case std::memory_order_relaxed: exch_relaxed(value, old, desired); break;
                default: assert(0);
                }
                return old;
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2, int>::type = 0>
            __device__ static inline bool compare_exchange_strong(T& value, T& expected, T desired, std::memory_order success, std::memory_order failure) noexcept {
                T old = T();
                switch (__stronger_order(success, failure)) {
                case std::memory_order_seq_cst: __mme_fence_sc();
                case std::memory_order_consume:
                case std::memory_order_acquire: cas_acquire(value, old, expected, desired); break;
                case std::memory_order_acq_rel: cas_acq_rel(value, old, expected, desired); break;
                case std::memory_order_release: cas_release(value, old, expected, desired); break;
                case std::memory_order_relaxed: cas_relaxed(value, old, expected, desired); break;
                default: assert(0);
                }
                bool ret = old == expected;
                expected = old;
                return ret;
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2, int>::type = 0>
            __device__ static inline T fetch_add(T& value,  T addend, std::memory_order order) noexcept {
                addend *= __scale_addend<T>();
                T ret = T();
                switch (order) {
                case std::memory_order_seq_cst: __mme_fence_sc();
                case std::memory_order_consume:
                case std::memory_order_acquire: add_acquire(value, ret, addend); break;
                case std::memory_order_acq_rel: add_acq_rel(value, ret, addend); break;
                case std::memory_order_release: add_release(value, ret, addend); break;
                case std::memory_order_relaxed: add_relaxed(value, ret, addend); break;
                default: assert(0);
                }
                return ret;
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2, int>::type = 0>
            __device__ static inline T fetch_and(T& value, T andend, std::memory_order order) noexcept {
                T ret = T();
                switch (order) {
                case std::memory_order_seq_cst: __mme_fence_sc();
                case std::memory_order_consume:
                case std::memory_order_acquire: and_acquire(value, ret, andend); break;
                case std::memory_order_acq_rel: and_acq_rel(value, ret, andend); break;
                case std::memory_order_release: and_release(value, ret, andend); break;
                case std::memory_order_relaxed: and_relaxed(value, ret, andend); break;
                default: assert(0);
                }
                return ret;
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2, int>::type = 0>
            __device__ static inline T fetch_or(T& value, T orend, std::memory_order order = std::memory_order_seq_cst) noexcept {
                T ret = T();
                switch (order) {
                case std::memory_order_seq_cst: __mme_fence_sc();
                case std::memory_order_consume:
                case std::memory_order_acquire: or_acquire(value, ret, orend); break;
                case std::memory_order_acq_rel: or_acq_rel(value, ret, orend); break;
                case std::memory_order_release: or_release(value, ret, orend); break;
                case std::memory_order_relaxed: or_relaxed(value, ret, orend); break;
                default: assert(0);
                }
                return ret;
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2, int>::type = 0>
            __device__ static inline T fetch_xor(T& value, T xorend, std::memory_order order = std::memory_order_seq_cst) noexcept {
                T ret = T();
                switch (order) {
                case std::memory_order_seq_cst: __mme_fence_sc();
                case std::memory_order_consume:
                case std::memory_order_acquire: xor_acquire(value, ret, xorend); break;
                case std::memory_order_acq_rel: xor_acq_rel(value, ret, xorend); break;
                case std::memory_order_release: xor_release(value, ret, xorend); break;
                case std::memory_order_relaxed: xor_relaxed(value, ret, xorend); break;
                default: assert(0);
                }
                return ret;
            }

#ifdef __has_cuda_mmio
            __device__ static inline T load(T const volatile& value, std::memory_order order) noexcept {
                T ret;
                switch (order) {
                case std::memory_order_seq_cst: __mme_fence_sc();
                case std::memory_order_consume:
                case std::memory_order_acquire: load_mmio(value, ret); __mme_fence(); break;
                case std::memory_order_relaxed: load_mmio(value, ret); break;
                default: assert(0);
                }
                return ret;
            }
            __device__ static inline void store(T volatile& value, T desired, std::memory_order order) noexcept {
                switch (order) {
                case std::memory_order_seq_cst: __mme_fence_sc(); break;
                case std::memory_order_release: __mme_fence();
                case std::memory_order_relaxed: break;
                default: assert(0);
                }
                store_mmio(value, desired);
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2, int>::type = 0>
            __device__ static inline T exchange(T volatile& value, T desired, std::memory_order order) noexcept {
                T old = T();
                switch (order) {
                case std::memory_order_seq_cst: __mme_fence_sc(); break;
                case std::memory_order_acq_rel:
                case std::memory_order_release: __mme_fence(); break;
                case std::memory_order_acquire:
                case std::memory_order_consume:
                case std::memory_order_relaxed: break;
                default: assert(0);
                }
                exch_mmio(value, old, desired);
                switch (order) {
                case std::memory_order_seq_cst:
                case std::memory_order_acq_rel:
                case std::memory_order_acquire:
                case std::memory_order_consume: __mme_fence(); break;
                case std::memory_order_release:
                case std::memory_order_relaxed: break;
                default: assert(0);
                }
                return old;
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2, int>::type = 0>
            __device__ static inline bool compare_exchange_strong(T volatile& value, T& expected, T desired, std::memory_order success, std::memory_order failure) noexcept {
                T old = T();
                auto order = __stronger_order(success, failure);
                switch (order) {
                case std::memory_order_seq_cst: __mme_fence_sc(); break;
                case std::memory_order_acq_rel:
                case std::memory_order_release: __mme_fence(); break;
                case std::memory_order_acquire:
                case std::memory_order_consume:
                case std::memory_order_relaxed: break;
                default: assert(0);
                }
                cas_mmio(value, old, expected, desired);
                switch (order) {
                case std::memory_order_seq_cst:
                case std::memory_order_acq_rel:
                case std::memory_order_acquire:
                case std::memory_order_consume: __mme_fence(); break;
                case std::memory_order_release:
                case std::memory_order_relaxed: break;
                default: assert(0);
                }
                bool ret = old == expected;
                expected = old;
                return ret;
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2, int>::type = 0>
            __device__ static inline T fetch_add(T volatile& value, typename std::conditional<std::is_pointer<T>::value, std::ptrdiff_t, T>::type addend, std::memory_order order) noexcept {
                addend *= __scale_addend<T>();
                T ret = T();
                switch (order) {
                case std::memory_order_seq_cst: __mme_fence_sc(); break;
                case std::memory_order_acq_rel:
                case std::memory_order_release: __mme_fence(); break;
                case std::memory_order_acquire:
                case std::memory_order_consume:
                case std::memory_order_relaxed: break;
                default: assert(0);
                }
                add_mmio(value, ret, addend);
                switch (order) {
                case std::memory_order_seq_cst:
                case std::memory_order_acq_rel:
                case std::memory_order_acquire:
                case std::memory_order_consume: __mme_fence(); break;
                case std::memory_order_release:
                case std::memory_order_relaxed: break;
                default: assert(0);
                }
                return ret;
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2, int>::type = 0>
            __device__ static inline T fetch_and(T volatile& value, T addend, std::memory_order order) noexcept {
                T ret = T();
                switch (order) {
                case std::memory_order_seq_cst: __mme_fence_sc(); break;
                case std::memory_order_acq_rel:
                case std::memory_order_release: __mme_fence(); break;
                case std::memory_order_acquire:
                case std::memory_order_consume:
                case std::memory_order_relaxed: break;
                default: assert(0);
                }
                and_mmio(value, ret, addend);
                switch (order) {
                case std::memory_order_seq_cst:
                case std::memory_order_acq_rel:
                case std::memory_order_acquire:
                case std::memory_order_consume: __mme_fence(); break;
                case std::memory_order_release:
                case std::memory_order_relaxed: break;
                default: assert(0);
                }
                return ret;
            }            
            template<typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2, int>::type = 0>
            __device__ static inline T fetch_or(T volatile& value, T addend, std::memory_order order) noexcept {
                T ret = T();
                switch (order) {
                case std::memory_order_seq_cst: __mme_fence_sc(); break;
                case std::memory_order_acq_rel:
                case std::memory_order_release: __mme_fence(); break;
                case std::memory_order_acquire:
                case std::memory_order_consume:
                case std::memory_order_relaxed: break;
                default: assert(0);
                }
                or_mmio(value, ret, addend);
                switch (order) {
                case std::memory_order_seq_cst:
                case std::memory_order_acq_rel:
                case std::memory_order_acquire:
                case std::memory_order_consume: __mme_fence(); break;
                case std::memory_order_release:
                case std::memory_order_relaxed: break;
                default: assert(0);
                }
                return ret;
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2, int>::type = 0>
            __device__ static inline T fetch_xor(T volatile& value, T addend, std::memory_order order) noexcept {
                T ret = T();
                switch (order) {
                case std::memory_order_seq_cst: __mme_fence_sc(); break;
                case std::memory_order_acq_rel:
                case std::memory_order_release: __mme_fence(); break;
                case std::memory_order_acquire:
                case std::memory_order_consume:
                case std::memory_order_relaxed: break;
                default: assert(0);
                }
                xor_mmio(value, ret, addend);
                switch (order) {
                case std::memory_order_seq_cst:
                case std::memory_order_acq_rel:
                case std::memory_order_acquire:
                case std::memory_order_consume: __mme_fence(); break;
                case std::memory_order_release:
                case std::memory_order_relaxed: break;
                default: assert(0);
                }
                return ret;
            }
#endif
        };
        
        template<class T>
        struct alignas(alignof(std::atomic<T>)) lockfree_device_atomic_impl {

            lockfree_device_atomic_impl() noexcept = default;

            T value;

            __device__ constexpr lockfree_device_atomic_impl(T desired) noexcept : value(desired) {
            }

            __device__ lockfree_device_atomic_impl(lockfree_device_atomic_impl const&) noexcept = delete;

            __device__ T load(std::memory_order order = std::memory_order_seq_cst) const noexcept {
                return __mme_std<T>::load(value, order);
            }
            __device__ void store(T desired, std::memory_order order = std::memory_order_seq_cst) noexcept {
                return __mme_std<T>::store(value, desired, order);
            }

            template<class Q /*may be T or volatile T*/>
            __device__ static T __exchange_8_16(Q& value, T desired, std::memory_order order) noexcept {

                T ret = __mme_std<T>::load(value, std::memory_order_relaxed);
                while (!__mme_std<T>::compare_exchange_strong(value, ret, desired, order, order))
                    ;
                return ret;
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) == 1 || sizeof(Q) == 2, int>::type = 0>
            __device__ T exchange(T desired, std::memory_order order = std::memory_order_seq_cst) noexcept {
                return __exchange_8_16(value, desired, order);
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2, int>::type = 0>
            __device__ T exchange(T desired, std::memory_order order = std::memory_order_seq_cst) noexcept {
                return __mme_std<T>::exchange(value, desired, order);
            }

            template<class Q /*may be T or volatile T*/>
            __device__ static bool __compare_exchange_strong_8_16(Q& value, T& expected, T desired, std::memory_order success, std::memory_order failure) noexcept {

                using this_T = typename std::conditional<std::is_volatile<Q>::value, volatile uint32_t, uint32_t>::type;

                this_T* const ptr = (this_T*)__semaphore_fixalign(value);
                auto const off = __semaphore_alignerr(value) * 8;
                auto const mask = int(T() - T(1)) << off;

                uint32_t old = expected << off, old_value;
                while (1) {
                    old_value = (old & mask) >> off;
                    if (old_value != expected)
                        break;
                    uint32_t const attempt = (old & ~mask) | (desired << off);
                    if (__mme_std<uint32_t>::compare_exchange_strong(*ptr, old, attempt, success, failure))
                        return true;
                }
                expected = old_value;
                return false;
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) == 1 || sizeof(Q) == 2, int>::type = 0>
            __device__ bool compare_exchange_strong(T& expected, T desired, std::memory_order success, std::memory_order failure) noexcept {
                return __compare_exchange_strong_8_16(value, expected, desired, success, failure);
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2, int>::type = 0>
            __device__ bool compare_exchange_strong(T& expected, T desired, std::memory_order success, std::memory_order failure) noexcept {
                return __mme_std<T>::compare_exchange_strong(value, expected, desired, success, failure);
            }

            template<class Q>
            __device__ static T __fetch_add_8_16(Q& value, T addend, std::memory_order order) noexcept {
                addend *= __scale_addend<Q>();
                T ret = __mme_std<T>::load(value, std::memory_order_relaxed);
                while (!__mme_std<T>::compare_exchange_strong(value, ret, ret + addend, order, order))
                    ;
                return ret;
            }
            template < typename Q = T, typename std::enable_if<(sizeof(Q) == 1 || sizeof(Q) == 2) && (std::is_integral<Q>::value || std::is_pointer<Q>::value), int>::type = 0>
            __device__ T fetch_add(T addend, std::memory_order order = std::memory_order_seq_cst) noexcept {
                return __fetch_add_8_16(value, addend, order);
            }
            template < typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2 && (std::is_integral<Q>::value || std::is_pointer<Q>::value), int>::type = 0>
            __device__ T fetch_add(T addend, std::memory_order order = std::memory_order_seq_cst) noexcept {
                return __mme_std<T>::fetch_add(value, addend, order);
            }

            template<class Q>
            __device__ static T __fetch_and_8_16(Q& value, T andend, std::memory_order order) noexcept {
                T ret = __mme_std<T>::load(value, std::memory_order_relaxed);
                while (!__mme_std<T>::compare_exchange_strong(value, ret, ret & andend, order, order))
                    ;
                return ret;
            }
            template < typename Q = T, typename std::enable_if<(sizeof(Q) == 1 || sizeof(Q) == 2) && std::is_integral<Q>::value, int>::type = 0>
            __device__ T fetch_and(T andend, std::memory_order order = std::memory_order_seq_cst) noexcept {
                return __fetch_and_8_16(value, andend, order);
            }
            template < typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2 && std::is_integral<Q>::value, int>::type = 0>
            __device__ T fetch_and(T andend, std::memory_order order = std::memory_order_seq_cst) noexcept {
                return __mme_std<T>::fetch_and(value, andend, order);
            }

            template<class Q>
            __device__ static T __fetch_or_8_16(Q& value, T orend, std::memory_order order) noexcept {
                T ret = __mme_std<T>::load(value, std::memory_order_relaxed);
                while (!__mme_std<T>::compare_exchange_strong(value, ret, ret | orend, order, order))
                    ;
                return ret;
            }
            template < typename Q = T, typename std::enable_if<(sizeof(Q) == 1 || sizeof(Q) == 2) && std::is_integral<Q>::value, int>::type = 0>
            __device__ T fetch_or(T orend, std::memory_order order = std::memory_order_seq_cst) noexcept {
                return __fetch_or_8_16(value, orend, order);
            }
            template < typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2 && std::is_integral<Q>::value, int>::type = 0>
            __device__ T fetch_or(T orend, std::memory_order order = std::memory_order_seq_cst) noexcept {
                return __mme_std<T>::fetch_or(value, orend, order);
            }

            template<class Q>
            __device__ static T __fetch_xor_8_16(Q& value, T xorend, std::memory_order order) noexcept {
                T ret = __mme_std<T>::load(value, std::memory_order_relaxed);
                while (!__mme_std<T>::compare_exchange_strong(value, ret, ret ^ xorend, order, order))
                    ;
                return ret;
            }
            template < typename Q = T, typename std::enable_if<(sizeof(Q) == 1 || sizeof(Q) == 2) && std::is_integral<Q>::value, int>::type = 0>
            __device__ T fetch_xor(T xorend, std::memory_order order = std::memory_order_seq_cst) noexcept {
                return __fetch_xor_8_16(value, xorend, order);
            }
            template < typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2 && std::is_integral<Q>::value, int>::type = 0>
            __device__ T fetch_xor(T xorend, std::memory_order order = std::memory_order_seq_cst) noexcept {
                return __mme_std<T>::fetch_xor(value, xorend, order);
            }

#ifdef __has_cuda_mmio
            __device__ T load(std::memory_order order = std::memory_order_seq_cst) const volatile noexcept {
                return __mme_std<T>::load(value, order);
            }
            __device__ void store(T desired, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
                return __mme_std<T>::store(value, desired, order);
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) == 1 || sizeof(Q) == 2, int>::type = 0>
            __device__ T exchange(T desired, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
                return __exchange_8_16(value, desired, order);
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2, int>::type = 0>
            __device__ T exchange(T desired, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
                return __mme_std<T>::exchange(value, desired, order);
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) == 1 || sizeof(Q) == 2, int>::type = 0>
            __device__ bool compare_exchange_strong(T& expected, T desired, std::memory_order success, std::memory_order failure) volatile noexcept {
                return __compare_exchange_strong_8_16(value, expected, desired, success, failure);
            }
            template<typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2, int>::type = 0>
            __device__ bool compare_exchange_strong(T& expected, T desired, std::memory_order success, std::memory_order failure) volatile noexcept {
                return __mme_std<T>::compare_exchange_strong(value, expected, desired, success, failure);
            }
            template < typename Q = T, typename std::enable_if<(sizeof(Q) == 1 || sizeof(Q) == 2) && (std::is_integral<Q>::value || std::is_pointer<Q>::value), int>::type = 0>
            __device__ T fetch_add(T addend, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
                return __fetch_add_8_16(value, addend, order);
            }
            template < typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2 && (std::is_integral<Q>::value || std::is_pointer<Q>::value), int>::type = 0>
            __device__ T fetch_add(T addend, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
                return __mme_std<T>::fetch_add(value, addend, order);
            }
            template < typename Q = T, typename std::enable_if<(sizeof(Q) == 1 || sizeof(Q) == 2) && std::is_integral<Q>::value, int>::type = 0>
            __device__ T fetch_and(T andend, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
                return __fetch_and_8_16(value, andend, order);
            }
            template < typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2 && std::is_integral<Q>::value, int>::type = 0>
            __device__ T fetch_and(T andend, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
                return __mme_std<T>::fetch_and(value, andend, order);
            }
            template < typename Q = T, typename std::enable_if<(sizeof(Q) == 1 || sizeof(Q) == 2) && std::is_integral<Q>::value, int>::type = 0>
            __device__ T fetch_or(T orend, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
                return __fetch_or_8_16(value, orend, order);
            }
            template < typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2 && std::is_integral<Q>::value, int>::type = 0>
            __device__ T fetch_or(T orend, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
                return __mme_std<T>::fetch_or(value, orend, order);
            }
            template < typename Q = T, typename std::enable_if<(sizeof(Q) == 1 || sizeof(Q) == 2) && std::is_integral<Q>::value, int>::type = 0>
            __device__ T fetch_xor(T xorend, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
                return __fetch_xor_8_16(value, xorend, order);
            }
            template < typename Q = T, typename std::enable_if<sizeof(Q) != 1 && sizeof(Q) != 2 && std::is_integral<Q>::value, int>::type = 0>
            __device__ T fetch_xor(T xorend, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
                return __mme_std<T>::fetch_xor(value, xorend, order);
            }
#endif
        };
        
        __host__ __device__ extern void __lock_by_address(void const*);
        __host__ __device__ extern void __lock_by_address(void const volatile*);
        __host__ __device__ extern void __unlock_by_address(void const*);
        __host__ __device__ extern void __unlock_by_address(void const volatile*);

        template<class T>
        struct locked_atomic_impl {

            using value_type = T;

            value_type __value;

            locked_atomic_impl() noexcept = default;

            __host__ __device__ constexpr locked_atomic_impl(T desired) noexcept : __value(desired) {
            }

            __host__ __device__ T load(std::memory_order order = std::memory_order_seq_cst) const noexcept {
                __lock_by_address(this);
                auto const ret = __value;
                __unlock_by_address(this);
                return ret;
            }
            __host__ __device__ void store(T desired, std::memory_order order = std::memory_order_seq_cst) noexcept {
                __lock_by_address(this);
                __value = desired;
                __unlock_by_address(this);
            }
            __host__ __device__ T exchange(T desired, std::memory_order order = std::memory_order_seq_cst) noexcept {
                __lock_by_address(this);
                auto const ret = __value;
                __value = desired;
                __unlock_by_address(this);
                return ret;
            }
            __host__ __device__ bool compare_exchange_strong(T& expected, T desired, std::memory_order success, std::memory_order failure) noexcept {
                __lock_by_address(this);
                auto const tmp = __value;
                expected = __value;
                if(tmp == expected) __value = desired;
                __unlock_by_address(this);
                return tmp == expected;
            }
            __host__ __device__ bool compare_exchange_strong(T& expected, T desired, std::memory_order success, std::memory_order failure) volatile noexcept {
                __lock_by_address(this);
                auto const tmp = __value;
                expected = __value;
                if (tmp == expected) __value = desired;
                __unlock_by_address(this);
                return tmp == expected;
            }
            template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value || std::is_pointer<Q>::value, int>::type = 0>
            __host__ __device__ T fetch_add(typename std::conditional<std::is_pointer<Q>::value, std::ptrdiff_t, T>::type addend, std::memory_order order = std::memory_order_seq_cst) noexcept {
                __lock_by_address(this);
                auto const ret = __value;
                __value += addend;
                __unlock_by_address(this);
                return ret;
            }
            template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value, int>::type = 0>
            __host__ __device__ T fetch_and(T andend, std::memory_order order = std::memory_order_seq_cst) noexcept {
                __lock_by_address(this);
                auto const ret = __value;
                __value &= andend;
                __unlock_by_address(this);
                return ret;
            }
            template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value, int>::type = 0>
            __host__ __device__ T fetch_or(T orend, std::memory_order order = std::memory_order_seq_cst) noexcept {
                __lock_by_address(this);
                auto const ret = __value;
                __value |= orend;
                __unlock_by_address(this);
                return ret;
            }
            template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value, int>::type = 0>
            __host__ __device__ T fetch_xor(T xorend, std::memory_order order = std::memory_order_seq_cst) noexcept {
                __lock_by_address(this);
                auto const ret = __value;
                __value ^= xorend;
                __unlock_by_address(this);
                return ret;
            }

#ifdef __has_cuda_mmio
            __host__ __device__ T load(std::memory_order order = std::memory_order_seq_cst) const volatile noexcept {
                __lock_by_address(this);
                auto const ret = __value;
                __unlock_by_address(this);
                return ret;
            }
            __host__ __device__ void store(T desired, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
                __lock_by_address(this);
                __value = desired;
                __unlock_by_address(this);
            }
            __host__ __device__ T exchange(T desired, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
                __lock_by_address(this);
                auto const ret = __value;
                __value = desired;
                __unlock_by_address(this);
                return ret;
            }
            template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value || std::is_pointer<Q>::value, int>::type = 0>
            __host__ __device__ T fetch_add(typename std::conditional<std::is_pointer<Q>::value, std::ptrdiff_t, T>::type addend, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
                __lock_by_address(this);
                auto const ret = __value;
                __value += addend;
                __unlock_by_address(this);
                return ret;
            }
            template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value, int>::type = 0>
            __host__ __device__ T fetch_and(T andend, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
                __lock_by_address(this);
                auto const ret = __value;
                __value &= andend;
                __unlock_by_address(this);
                return ret;
            }
            template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value, int>::type = 0>
            __host__ __device__ T fetch_or(T orend, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
                __lock_by_address(this);
                auto const ret = __value;
                __value |= orend;
                __unlock_by_address(this);
                return ret;
            }
            template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value, int>::type = 0>
            __host__ __device__ T fetch_xor(T xorend, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
                __lock_by_address(this);
                auto const ret = __value;
                __value ^= xorend;
                __unlock_by_address(this);
                return ret;
            }
#endif
        };

        template<typename T> struct is_always_lock_free { enum { value = false }; };
        template<> struct is_always_lock_free<bool> { enum { value = 2 == ATOMIC_BOOL_LOCK_FREE }; };
        template<> struct is_always_lock_free<char> { enum { value = 2 == ATOMIC_CHAR_LOCK_FREE }; };
        template<> struct is_always_lock_free<signed char> { enum { value = 2 == ATOMIC_CHAR_LOCK_FREE }; };
        template<> struct is_always_lock_free<unsigned char> { enum { value = 2 == ATOMIC_CHAR_LOCK_FREE }; };
        template<> struct is_always_lock_free<char16_t> { enum { value = 2 == ATOMIC_CHAR16_T_LOCK_FREE }; };
        template<> struct is_always_lock_free<char32_t> { enum { value = 2 == ATOMIC_CHAR32_T_LOCK_FREE }; };
        template<> struct is_always_lock_free<wchar_t> { enum { value = 2 == ATOMIC_WCHAR_T_LOCK_FREE }; };
        template<> struct is_always_lock_free<short> { enum { value = 2 == ATOMIC_SHORT_LOCK_FREE }; };
        template<> struct is_always_lock_free<unsigned short> { enum { value = 2 == ATOMIC_SHORT_LOCK_FREE }; };
        template<> struct is_always_lock_free<int> { enum { value = 2 == ATOMIC_INT_LOCK_FREE }; };
        template<> struct is_always_lock_free<unsigned int> { enum { value = 2 == ATOMIC_INT_LOCK_FREE }; };
        template<> struct is_always_lock_free<long> { enum { value = 2 == ATOMIC_LONG_LOCK_FREE }; };
        template<> struct is_always_lock_free<unsigned long> { enum { value = 2 == ATOMIC_LONG_LOCK_FREE }; };
        template<> struct is_always_lock_free<long long> { enum { value = 2 == ATOMIC_LLONG_LOCK_FREE }; };
        template<> struct is_always_lock_free<unsigned long long> { enum { value = 2 == ATOMIC_LLONG_LOCK_FREE }; };
        template<typename T> struct is_always_lock_free<T*> { enum { value = 2 == ATOMIC_POINTER_LOCK_FREE }; };
        template<> struct is_always_lock_free<std::nullptr_t> { enum { value = 2 == ATOMIC_POINTER_LOCK_FREE }; };

        static_assert(is_always_lock_free<int>::value, "This implementation does not work if atomic<int> is not lock-free");

        template<class T>
        using __atomic_base = typename std::conditional<is_always_lock_free<T>::value,
#ifdef __CUDA_ARCH__
            lockfree_device_atomic_impl<T>,
#else
            std::atomic<T>,
#endif
            locked_atomic_impl<T >> ::type;
    }

    // 32.4, order and consistency 
    using memory_order = std::memory_order;
    template <class T> T kill_dependency(T t) noexcept { return t; }

    // 32.5, lock-free property 
    //#define ATOMIC_BOOL_LOCK_FREE unspecified 
    //#define ATOMIC_CHAR_LOCK_FREE unspecified 
    //#define ATOMIC_CHAR16_T_LOCK_FREE unspecified 
    //#define ATOMIC_CHAR32_T_LOCK_FREE unspecified 
    //#define ATOMIC_WCHAR_T_LOCK_FREE unspecified 
    //#define ATOMIC_SHORT_LOCK_FREE unspecified 
    //#define ATOMIC_INT_LOCK_FREE unspecified 
    //#define ATOMIC_LONG_LOCK_FREE unspecified 
    //#define ATOMIC_LLONG_LOCK_FREE unspecified 
    //#define ATOMIC_POINTER_LOCK_FREE unspecified

    // 32.6, atomic
    // 32.6.3, partial specialization for pointers
    template <typename T>
    struct atomic {
        static_assert(std::is_trivially_copyable<T>::value, "The atomic<T> template requires trivially-copyable T");

        using value_type = T;
        using difference_type = typename std::conditional<std::is_pointer<T>::value, ptrdiff_t, T>::type;
        static constexpr bool is_always_lock_free = details::is_always_lock_free<T>::value;

        __host__ __device__ bool is_lock_free() const noexcept { return is_always_lock_free; }

        __host__ __device__ void store(T desired, std::memory_order order = std::memory_order_seq_cst) noexcept {
            _base.store(desired, order);
        }
        __host__ __device__ T load(std::memory_order order = std::memory_order_seq_cst) const noexcept {
            return _base.load(order);
        }
        __host__ __device__ operator T() const noexcept {
            return _base.load();
        }
        __host__ __device__ T exchange(T desired, std::memory_order order = std::memory_order_seq_cst) noexcept {
            return _base.exchange(desired, order);
        }
        __host__ __device__ bool compare_exchange_weak(T& expected, T desired, std::memory_order success, std::memory_order failure) noexcept {
            return compare_exchange_strong(expected, desired, success, failure);
        }
        __host__ __device__ bool compare_exchange_strong(T& expected, T desired, std::memory_order success, std::memory_order failure) noexcept {
            return _base.compare_exchange_strong(expected, desired, success, failure);
        }
        __host__ __device__ bool compare_exchange_weak(T& expected, T desired, std::memory_order order = std::memory_order_seq_cst) noexcept {
            return compare_exchange_weak(expected, desired, order, order);
        }
        __host__ __device__ bool compare_exchange_strong(T& expected, T desired, std::memory_order order = std::memory_order_seq_cst) noexcept {
            return _base.compare_exchange_strong(expected, desired, order, order);
        }

        details::__atomic_base<T> _base;

        atomic() noexcept = default;
        __host__ __device__ constexpr atomic(T desired) noexcept : _base(desired) { }
        __host__ __device__ atomic(atomic const&) = delete;
        __host__ __device__ atomic& operator=(atomic const&) = delete;
        __host__ __device__ T operator=(T desired) noexcept {
            _base.store(desired);
            return desired;
        }

        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value || std::is_pointer<Q>::value, int>::type = 0>
        __host__ __device__ T fetch_add(difference_type addend, std::memory_order order = std::memory_order_seq_cst) noexcept {
            return _base.fetch_add(addend, order);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value || std::is_pointer<Q>::value, int>::type = 0>
        __host__ __device__ T fetch_sub(difference_type addend, std::memory_order order = std::memory_order_seq_cst) noexcept {
            return _base.fetch_add(T() - addend, order);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value, int>::type = 0>
        __host__ __device__ T fetch_and(T andend, std::memory_order order = std::memory_order_seq_cst) noexcept {
            return _base.fetch_and(andend, order);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value, int>::type = 0>
        __host__ __device__ T fetch_or(T orend, std::memory_order order = std::memory_order_seq_cst) noexcept {
            return _base.fetch_or(orend, order);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value, int>::type = 0>
        __host__ __device__ T fetch_xor(T xorend, std::memory_order order = std::memory_order_seq_cst) noexcept {
            return _base.fetch_xor(xorend, order);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value || std::is_pointer<Q>::value, int>::type = 0>
        __host__ __device__ T operator++(int) noexcept {
            return _base.fetch_add(1);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value || std::is_pointer<Q>::value, int>::type = 0>
        __host__ __device__ T operator--(int) noexcept {
            return _base.fetch_sub(1);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value || std::is_pointer<Q>::value, int>::type = 0>
        __host__ __device__ T operator++() noexcept {
            return _base.fetch_add(1) + 1;
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value || std::is_pointer<Q>::value, int>::type = 0>
        __host__ __device__ T operator--() noexcept {
            return _base.fetch_sub(1) - 1;
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value || std::is_pointer<Q>::value, int>::type = 0>
        __host__ __device__ T operator+=(difference_type addend) noexcept {
            return _base.fetch_add(addend);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value || std::is_pointer<Q>::value, int>::type = 0>
        __host__ __device__ T operator-=(difference_type addend) noexcept {
            return _base.fetch_sub(addend);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value, int>::type = 0>
        __host__ __device__ T operator&=(T andend) noexcept {
            return _base.fetch_and(andend);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value, int>::type = 0>
        __host__ __device__ T operator|=(T orend) noexcept {
            return _base.fetch_or(orend);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value, int>::type = 0>
        __host__ __device__ T operator^=(T xorend) noexcept {
            return _base.fetch_xor(xorend);
        }

#ifdef __has_cuda_mmio
        __host__ __device__ bool is_lock_free() const volatile noexcept { return is_always_lock_free; }

        __host__ __device__ void store(T desired, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
            _base.store(desired, order);
        }
        __host__ __device__ T load(std::memory_order order = std::memory_order_seq_cst) const volatile noexcept {
            return _base.load(order);
        }
        __host__ __device__ operator T() const volatile noexcept {
            return _base.load();
        }
        __host__ __device__ T exchange(T desired, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
            return _base.exchange(desired, order);
        }
        __host__ __device__ bool compare_exchange_weak(T& expected, T desired, std::memory_order success, std::memory_order failure) volatile noexcept {
            return compare_exchange_strong(expected, desired, success, failure);
        }
        __host__ __device__ bool compare_exchange_strong(T& expected, T desired, std::memory_order success, std::memory_order failure) volatile noexcept {
            return _base.compare_exchange_strong(expected, desired, success, failure);
        }
        __host__ __device__ bool compare_exchange_weak(T& expected, T desired, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
            return compare_exchange_weak(expected, desired, order, order);
        }
        __host__ __device__ bool compare_exchange_strong(T& expected, T desired, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
            return _base.compare_exchange_strong(expected, desired, order, order);
        }

        __host__ __device__ atomic& operator=(atomic const&) volatile = delete;
        __host__ __device__ T operator=(T desired) volatile noexcept {
            _base.store(desired);
            return desired;
        }

        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value || std::is_pointer<Q>::value, int>::type = 0>
        __host__ __device__ T fetch_add(difference_type addend, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
            return _base.fetch_add(addend, order);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value || std::is_pointer<Q>::value, int>::type = 0>
        __host__ __device__ T fetch_sub(difference_type addend, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
            return _base.fetch_add(T() - addend, order);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value, int>::type = 0>
        __host__ __device__ T fetch_and(T andend, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
            return _base.fetch_and(andend, order);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value, int>::type = 0>
        __host__ __device__ T fetch_or(T orend, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
            return _base.fetch_or(orend, order);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value, int>::type = 0>
        __host__ __device__ T fetch_xor(T xorend, std::memory_order order = std::memory_order_seq_cst) volatile noexcept {
            return _base.fetch_xor(xorend, order);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value || std::is_pointer<Q>::value, int>::type = 0>
        __host__ __device__ T operator++(int) volatile noexcept {
            return _base.fetch_add(1);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value || std::is_pointer<Q>::value, int>::type = 0>
        __host__ __device__ T operator--(int) volatile noexcept {
            return _base.fetch_sub(1);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value || std::is_pointer<Q>::value, int>::type = 0>
        __host__ __device__ T operator++() volatile noexcept {
            return _base.fetch_add(1) + 1;
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value || std::is_pointer<Q>::value, int>::type = 0>
        __host__ __device__ T operator--() volatile noexcept {
            return _base.fetch_sub(1) - 1;
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value || std::is_pointer<Q>::value, int>::type = 0>
        __host__ __device__ T operator+=(difference_type addend) volatile noexcept {
            return _base.fetch_add(addend);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value || std::is_pointer<Q>::value, int>::type = 0>
        __host__ __device__ T operator-=(difference_type addend) volatile noexcept {
            return _base.fetch_sub(addend);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value, int>::type = 0>
        __host__ __device__ T operator&=(T andend) volatile noexcept {
            return _base.fetch_and(andend);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value, int>::type = 0>
        __host__ __device__ T operator|=(T orend) volatile noexcept {
            return _base.fetch_or(orend);
        }
        template < typename Q = T, typename std::enable_if<std::is_integral<Q>::value, int>::type = 0>
        __host__ __device__ T operator^=(T xorend) volatile noexcept {
            return _base.fetch_xor(xorend);
        }
#endif
    };

    // 32.7, non-member functions
    template<class T> __host__ __device__ bool atomic_is_lock_free(const atomic<T>* a) noexcept {
        return a->is_lock_free();
    }
    template<class T> __host__ __device__ void atomic_init(atomic<T>* a, typename atomic<T>::value_type desired) noexcept {
        new (a) atomic<T>(desired);
    }
    template<class T> __host__ __device__ void atomic_store(atomic<T>* a, typename atomic<T>::value_type desired) noexcept {
        a->store(desired);
    }
    template<class T> __host__ __device__ void atomic_store_explicit(atomic<T>* a, typename atomic<T>::value_type desired, memory_order order) noexcept {
        a->store(desired, order);
    }
    template<class T> __host__ __device__ T atomic_load(const atomic<T>* a) noexcept {
        return a->load();
    }
    template<class T> __host__ __device__ T atomic_load_explicit(const atomic<T>* a, memory_order order) noexcept {
        return a->load(order);
    }
    template<class T> __host__ __device__ T atomic_exchange(atomic<T>* a, typename atomic<T>::value_type desired) noexcept {
        return a->exchange(desired);
    }
    template<class T> __host__ __device__ T atomic_exchange_explicit(atomic<T>* a, typename atomic<T>::value_type desired, memory_order order) noexcept {
        return a->exchange(desired, order);
    }
    template<class T> __host__ __device__ bool atomic_compare_exchange_weak(atomic<T>* a, typename atomic<T>::value_type* expected, typename atomic<T>::value_type desired) noexcept {
        return a->compare_exchange_weak(expected, desired);
    }
    template<class T> __host__ __device__ bool atomic_compare_exchange_strong(atomic<T>* a, typename atomic<T>::value_type* expected, typename atomic<T>::value_type desired) noexcept {
        return a->compare_exchange_strong(expected, desired);
    }
    template<class T> __host__ __device__ bool atomic_compare_exchange_weak_explicit(atomic<T>* a, typename atomic<T>::value_type* expected, typename atomic<T>::value_type desired, memory_order success, memory_order failure) noexcept {
        return a->compare_exchange_weak(expected, desired, success, failure);
    }
    template<class T> __host__ __device__ bool atomic_compare_exchange_strong_explicit(atomic<T>* a, typename atomic<T>::value_type* expected, typename atomic<T>::value_type desired, memory_order success, memory_order failure) noexcept {
        return a->compare_exchange_strong(expected, desired, success, failure);
    }
    template <class T> __host__ __device__ T atomic_fetch_add(atomic<T>* a, typename atomic<T>::difference_type addend) noexcept {
        return a->fetch_add(addend);
    }
    template <class T> __host__ __device__ T atomic_fetch_add_explicit(atomic<T>* a, typename atomic<T>::difference_type addend, memory_order order) noexcept {
        return a->fetch_add(addend, order);
    }
    template <class T> __host__ __device__ T atomic_fetch_sub(atomic<T>* a, typename atomic<T>::difference_type addend) noexcept {
        return a->fetch_sub(addend);
    }
    template <class T> __host__ __device__ T atomic_fetch_sub_explicit(atomic<T>* a, typename atomic<T>::difference_type addend, memory_order order) noexcept {
        return a->fetch_sub(addend, order);
    }
    template <class T> __host__ __device__ T atomic_fetch_and(atomic<T>* a, typename atomic<T>::value_type andend) noexcept {
        return a->fetch_and(andend);
    }
    template <class T> __host__ __device__ T atomic_fetch_and_explicit(atomic<T>* a, typename atomic<T>::value_type andend, memory_order order) noexcept {
        return a->fetch_and(andend, order);
    }
    template <class T> __host__ __device__ T atomic_fetch_or(atomic<T>* a, typename atomic<T>::value_type orend) noexcept {
        return a->fetch_or(orend);
    }
    template <class T> __host__ __device__ T atomic_fetch_or_explicit(atomic<T>* a, typename atomic<T>::value_type orend, memory_order order) noexcept {
        return a->fetch_or(orend, order);
    }
    template <class T> __host__ __device__ T atomic_fetch_xor(atomic<T>* a, typename atomic<T>::value_type xorend) noexcept {
        return a->fetch_xor(xorend);
    }
    template <class T> __host__ __device__ T atomic_fetch_xor_explicit(atomic<T>* a, typename atomic<T>::value_type xorend, memory_order order) noexcept {
        return a->fetch_xor(xorend, order);
    }

#ifdef __has_cuda_mmio
    template<class T> __host__ __device__ bool atomic_is_lock_free(const volatile atomic<T>* a) noexcept {
        return a->is_lock_free();
    }
    template<class T> __host__ __device__ void atomic_init(volatile atomic<T>* a, typename atomic<T>::value_type desired) noexcept {
        new (a) volatile atomic<T>(desired);
    }
    template<class T> __host__ __device__ void atomic_store(volatile atomic<T>* a, typename atomic<T>::value_type desired) noexcept {
        a->store(desired);
    }
    template<class T> __host__ __device__ void atomic_store_explicit(volatile atomic<T>* a, typename atomic<T>::value_type desired, memory_order order) noexcept {
        a->store(desired, order);
    }
    template<class T> __host__ __device__ T atomic_load(const volatile atomic<T>* a) noexcept {
        return a->load();
    }
    template<class T> __host__ __device__ T atomic_load_explicit(const volatile atomic<T>* a, memory_order order) noexcept {
        return a->load(order);
    }
    template<class T> __host__ __device__ T atomic_exchange(volatile atomic<T>* a, typename atomic<T>::value_type desired) noexcept {
        return a->exchange(desired);
    }
    template<class T> __host__ __device__ T atomic_exchange_explicit(volatile atomic<T>* a, typename atomic<T>::value_type desired, memory_order order) noexcept {
        return a->exchange(desired, order);
    }
    template<class T> __host__ __device__ bool atomic_compare_exchange_weak(volatile atomic<T>* a, typename atomic<T>::value_type* expected, typename atomic<T>::value_type desired) noexcept {
        return a->compare_exchange_weak(expected, desired);
    }
    template<class T> __host__ __device__ bool atomic_compare_exchange_strong(volatile atomic<T>* a, typename atomic<T>::value_type* expected, typename atomic<T>::value_type desired) noexcept {
        return a->compare_exchange_strong(expected, desired);
    }
    template<class T> __host__ __device__ bool atomic_compare_exchange_weak_explicit(volatile atomic<T>* a, typename atomic<T>::value_type* expected, typename atomic<T>::value_type desired, memory_order success, memory_order failure) noexcept {
        return a->compare_exchange_weak(expected, desired, success, failure);
    }
    template<class T> __host__ __device__ bool atomic_compare_exchange_strong_explicit(volatile atomic<T>* a, typename atomic<T>::value_type* expected, typename atomic<T>::value_type desired, memory_order success, memory_order failure) noexcept {
        return a->compare_exchange_strong(expected, desired, success, failure);
    }
    template <class T> __host__ __device__ T atomic_fetch_add(volatile atomic<T>* a, typename atomic<T>::difference_type addend) noexcept {
        return a->fetch_add(addend);
    }
    template <class T> __host__ __device__ T atomic_fetch_add_explicit(volatile atomic<T>* a, typename atomic<T>::difference_type addend, memory_order order) noexcept {
        return a->fetch_add(addend, order);
    }
    template <class T> __host__ __device__ T atomic_fetch_sub(volatile atomic<T>* a, typename atomic<T>::difference_type addend) noexcept {
        return a->fetch_sub(addend);
    }
    template <class T> __host__ __device__ T atomic_fetch_sub_explicit(volatile atomic<T>* a, typename atomic<T>::difference_type addend, memory_order order) noexcept {
        return a->fetch_sub(addend, order);
    }
    template <class T> __host__ __device__ T atomic_fetch_and(volatile atomic<T>* a, typename atomic<T>::value_type andend) noexcept {
        return a->fetch_and(andend);
    }
    template <class T> __host__ __device__ T atomic_fetch_and_explicit(volatile atomic<T>* a, typename atomic<T>::value_type andend, memory_order order) noexcept {
        return a->fetch_and(andend, order);
    }
    template <class T> __host__ __device__ T atomic_fetch_or(volatile atomic<T>* a, typename atomic<T>::value_type orend) noexcept {
        return a->fetch_or(orend);
    }
    template <class T> __host__ __device__ T atomic_fetch_or_explicit(volatile atomic<T>* a, typename atomic<T>::value_type orend, memory_order order) noexcept {
        return a->fetch_or(orend, order);
    }
    template <class T> __host__ __device__ T atomic_fetch_xor(volatile atomic<T>* a, typename atomic<T>::value_type xorend) noexcept {
        return a->fetch_xor(xorend);
    }
    template <class T> __host__ __device__ T atomic_fetch_xor_explicit(volatile atomic<T>* a, typename atomic<T>::value_type xorend, memory_order order) noexcept {
        return a->fetch_xor(xorend, order);
    }
#endif

    // 32.6.1, initialization
    //#define ATOMIC_VAR_INIT(value) {value}

    // 32.3, type aliases 
    using atomic_bool = atomic<bool>; 
    using atomic_char = atomic<char>; 
    using atomic_schar = atomic<signed char>; 
    using atomic_uchar = atomic<unsigned char>; 
    using atomic_short = atomic<short>; 
    using atomic_ushort = atomic<unsigned short>; 
    using atomic_int = atomic<int>; 
    using atomic_uint = atomic<unsigned int>; 
    using atomic_long = atomic<long>; 
    using atomic_ulong = atomic<unsigned long>; 
    using atomic_llong = atomic<long long>; 
    using atomic_ullong = atomic<unsigned long long>; 
    using atomic_char16_t = atomic<char16_t>; 
    using atomic_char32_t = atomic<char32_t>; 
    using atomic_wchar_t = atomic<wchar_t>;
    using atomic_int8_t = atomic<int8_t>; 
    using atomic_uint8_t = atomic<uint8_t>; 
    using atomic_int16_t = atomic<int16_t>; 
    using atomic_uint16_t = atomic<uint16_t>; 
    using atomic_int32_t = atomic<int32_t>; 
    using atomic_uint32_t = atomic<uint32_t>; 
    using atomic_int64_t = atomic<int64_t>; 
    using atomic_uint64_t = atomic<uint64_t>;
    using atomic_int_least8_t = atomic<int_least8_t>; 
    using atomic_uint_least8_t = atomic<uint_least8_t>; 
    using atomic_int_least16_t = atomic<int_least16_t>; 
    using atomic_uint_least16_t = atomic<uint_least16_t>; 
    using atomic_int_least32_t = atomic<int_least32_t>; 
    using atomic_uint_least32_t = atomic<uint_least32_t>; 
    using atomic_int_least64_t = atomic<int_least64_t>; 
    using atomic_uint_least64_t = atomic<uint_least64_t>;
    using atomic_int_fast8_t = atomic<int_fast8_t>; 
    using atomic_uint_fast8_t = atomic<uint_fast8_t>; 
    using atomic_int_fast16_t = atomic<int_fast16_t>; 
    using atomic_uint_fast16_t = atomic<uint_fast16_t>;
    using atomic_int_fast32_t = atomic<int_fast32_t>; 
    using atomic_uint_fast32_t = atomic<uint_fast32_t>; 
    using atomic_int_fast64_t = atomic<int_fast64_t>; 
    using atomic_uint_fast64_t = atomic<uint_fast64_t>;
    using atomic_intptr_t = atomic<intptr_t>; 
    using atomic_uintptr_t = atomic<uintptr_t>; 
    using atomic_size_t = atomic<size_t>; 
    using atomic_ptrdiff_t = atomic<ptrdiff_t>; 
    using atomic_intmax_t = atomic<intmax_t>; 
    using atomic_uintmax_t = atomic<uintmax_t>;

    // 32.8, flag type and operations
    struct atomic_flag {
        __host__ __device__ bool test_and_set(memory_order order = std::memory_order_seq_cst) noexcept {
            return _base.exchange(1, order) == 0;
        }
        __host__ __device__ void clear(memory_order order = std::memory_order_seq_cst) noexcept {
            return _base.store(0, order);
        }

        atomic<int> _base;

        atomic_flag() noexcept = default;
        atomic_flag(const atomic_flag&) = delete; 
        atomic_flag& operator=(const atomic_flag&) = delete; 

#ifdef __has_cuda_mmio
        __host__ __device__ bool test_and_set(memory_order order = std::memory_order_seq_cst) volatile noexcept {
            return _base.exchange(1, order) == 0;
        }
        __host__ __device__ void clear(memory_order order = std::memory_order_seq_cst) volatile noexcept {
            return _base.store(0, order);
        }
        atomic_flag& operator=(const atomic_flag&) volatile = delete;
#endif
    };
    __host__ __device__ inline bool atomic_flag_test_and_set(atomic_flag* f) noexcept {
        return f->test_and_set();
    }
    __host__ __device__ inline bool atomic_flag_test_and_set_explicit(atomic_flag* f, std::memory_order order) noexcept {
        return f->test_and_set(order);
    }
    __host__ __device__ inline void atomic_flag_clear(atomic_flag* f) noexcept {
        f->clear();
    }
    __host__ __device__ inline void atomic_flag_clear_explicit(atomic_flag* f, std::memory_order order) noexcept {
        f->clear(order);
    }
#ifdef __has_cuda_mmio
    __host__ __device__ inline bool atomic_flag_test_and_set(volatile atomic_flag* f) noexcept {
        return f->test_and_set();
    }
    __host__ __device__ inline bool atomic_flag_test_and_set_explicit(volatile atomic_flag* f, std::memory_order order) noexcept {
        return f->test_and_set(order);
    }
    __host__ __device__ inline void atomic_flag_clear(volatile atomic_flag* f) noexcept {
        f->clear();
    }
    __host__ __device__ inline void atomic_flag_clear_explicit(volatile atomic_flag* f, std::memory_order order) noexcept {
        f->clear(order);
    }
#endif
    //Defined by host.
    //#define ATOMIC_FLAG_INIT {0}

    // 32.9, fences
    extern "C" __host__ __device__ inline void __cuda_atomic_thread_fence(std::memory_order order) {
#ifdef __CUDA_ARCH__
        switch (order) {
        case std::memory_order_seq_cst: details::__mme_fence_sc(); break;
        case std::memory_order_consume:
        case std::memory_order_acquire:
        case std::memory_order_acq_rel:
        case std::memory_order_release: details::__mme_fence(); break;
        case std::memory_order_relaxed: break;
        default: assert(0);
        }
#else
        std::atomic_thread_fence(order);
#endif
    }
    #define atomic_thread_fence __cuda_atomic_thread_fence

    extern "C" __host__ __device__ inline void __cuda_atomic_signal_fence(memory_order order) noexcept {
#ifdef __CUDA_ARCH__
        details::__mme_fence_signal();
#else
        std::atomic_signal_fence(order);
#endif
    }
    #define atomic_signal_fence __cuda_atomic_signal_fence

} } }

#endif //__CUDA_ATOMIC_HPP__
